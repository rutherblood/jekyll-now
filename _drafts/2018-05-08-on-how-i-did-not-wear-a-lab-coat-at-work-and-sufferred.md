---
published: false
---
One of my current roommates works at a government centre for disease control research here at Currey Road. He is a biotechnology graduate and his work most requires him to collect data in experiments according to a strictly defined protocol. The word "protocol" here is key. 

The experiments have to be performed according to a pre-established principle. Protocols become important in a setting where there is a high chance of a minor mistake invalidating the entire set of results which draw on the data generated by these experiments. These biotech protocols usually require years to master as they need one to work with and control unstable chemicals, living tissue, dirty vessels, one's own body, the environment itself. Dirt from under your dirty fingernails can invalidate six months of your work in a second. My roomate over the course of the year has worked on just two of these experiments.

Lately, I have come to take an interest in such matters because of a development in my own work. My current project requires me to work with the Touch API in browsers: filtering unrequired palm touches when one writes with a stylus on an HTML5 canvas on touch devices in the natural manner one writes with on a sheet of paper. My goto device on which I have been testing and for which I am polishing my results is an iPad 10.

A solution to the problem lies with machine learning. Inspired by [this](http://juliaschwarz.net/assets/palm-rejection/schwarz-chi14-palmrejection.pdf "Paper on palm rejection using probabilistic UI") paper I set out to train a decision tree in the matters of palm and stylus. How one trains a decision tree is by generating a relational (tabular) dataset with various characterstics of a touch contact (when I say _touch contact_, what I mean is a single touch gesture right from the start upon first contact, to the movements and to the end when you pick up your finger, stylus, palm or whatever you've set to rest on that screen) like the distance of the start contact from its neighbors startpoints within a given timewindow, mean radius and the like. Now, each entry in this dataset is labelled as _palm_ or _stylus_ and handed over to a decision-tree generation algorithm like the C4.5 or CART to juggle with. I take this decision tree, encode it to if-else clauses and use it in my final product. How I collect the data is by taking an HTML5 canvas, generating a random 10px circle anywhere on it and asking a labrat to try initiating a stroke from inside the circle using a stylus while naturally resting her palm on the iPad like she would on a sheet of paper. Simple right?

Except it's not if **YOU'RE AN UNDISCIPLINED AND INCONSISTENT FUCK**. You see I am that way and this is a protocol. I have the feeling and I think I am mostly right that when it comes to modern statistical machine learning things are not quite different from my roommate's experiments with stemcells. Machine learning is not the magical device of highly-paid wizards who sway their wand at any problem and the problem goes away, no sir. Generating the training set with a protocol, transforming it, feeding it to the algorithm with the perfect among the million variations of parameters you could tune, and testing the results with the perfect statistical formulae, mastering this entire chain of protocols is where the millions bucks in salaries lie at. It always helps to be a mathematician ofcourse. 

Let me tell you what I'm ranting about and where I screwed up:

1. I collected multiple data sets for the various kind of gestures like panning, zooming and others which did not fall within the category of my prime target (i.e. stylus or palm) and which needed to be positively filtered. I mixed them with my stylus/palm data set to train the decision tree. Except I didn't label these data-sets descriptively and ended up with a hodge podge of them, taking a guess, mixing incompatible data-sets and ended up with unwanted decision trees. Do not ask me how many times I repeated this same mistake.

2. My test scripts compute their own characterstics on the raw touch event data that comes in. These characterstics need to strictly be computed in the same way the characterstics were computed during my training. Turns out I initially wasted a lot of time due to bugs in both my training code, testing code and inconsistencies in my own thinking which led to different ways some attributes/characterstics were computed while training and while testing. 

Looking at the big picture, there are questions which come to mind now that I have been working with statistics and in an experimental framework: how can one _definitely_ tell if the inferences one makes from a dataset is right? Is testing the hypothesis on an unlooked dataset the only way? Is a _proof_ the hypothesis possible in any way? Turns out this is the [problem of induction](https://plato.stanford.edu/entries/induction-problem/ "SEP article on the Problem Of Induction") as put by David Hume.

See you tomorrow,
Avi.

P.S.: **MORAL OF THE STORY:** It nevers helps to be not disciplined. Always be disciplined. Chao.